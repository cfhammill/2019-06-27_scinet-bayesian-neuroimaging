---
title: "Bayesian analysis of Neuroimaging data with R"
author: "<b>Chris Hammill</b><br>Jason Lerch"
date: "2019-06-27"
output:
  xaringan::moon_reader:
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style>
.remark-code { 
  font-size: 75%;
}

.remark-slide-scaler {
  overflow: scroll;
}

.small {
  font-size: 65%;
}

.remark-slide-number {
  font-size: 10pt;
  margin-bottom: -11.6px;
  margin-right: 10px;
  color: #FFFFFF; /* white */
  opacity: 1; /* default: 0.5 */
}
</style>


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(knitr)
opts_chunk$set(dpi=200, fig.height=5.5, fig.width=10)
#opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


# Outline

1. Quick overview of algorithms and datasets used today
1. Anatomy and hierarchies
1. Massively univariate classical statistics
1. Meet Reverend Thomas
1. A simple model
1. A more involved model
1. Diagnostics
1. A complex example (if we have time)
1. ... 
1. Profit?

---

# Following Along

- To follow along with today's session you will need to either:
- run R with `c("knitr", "tidyverse", "forcats", "ggplot2", "broom", "data.tree", "treemap", "rstanarm", "bayesplot", "pheatmap", "ggrepel")` on your laptop
- use R from our singularity container

---

# Get these slides and code

- From a terminal run:

```{r, engine = "bash", eval = FALSE}
git clone https://github.com/Mouse-Imaging-Centre/Scinet-SS-bayesian-neuroimaging-R
cd Scinet-SS-bayesian-neuroimaging-R

## cp in some extra files <use scp if you're working on your own laptop>
cp /scinet/course/ss2019/3/8_bayesianmri/ADNI2_BL_MAGeT_Hippocampus_subfields.csv .
cp /scinet/course/ss2019/3/8_bayesianmri/flathierarchy.Rds .
cp /scinet/course/ss2019/3/8_bayesianmri/twolevelhierarchy.Rds .
```
---

# Singularity set up

- First ssh to niagara and set your port to forward

```{r, engine = "bash", eval = FALSE}
ssh -L8787:localhost:<your-port> <you>@teach.scinet.utoronto.ca
debugjob
ssh -f -N -T -R <your-port>:localhost:<your-port> <you>@teach.scinet.utoronto.ca
```

- To use the singularity container you will need to set an environment variable with a temporary RStudio password. You will also
  need a temporary directory on SCRATCH, this assumes you don't have a directory with this name already

```{r, engine = "bash", eval = FALSE}
export RSTUDIO_PASSWORD="dont_use_this_fake_password"

mkdir $SCRATCH/tmp #Ignore the error if this exists already
module load singularity
```

---

# Run the container

- Run the container <this command will remain running>, can be sent to the background

```{r, engine = "bash", eval = FALSE}
singularity exec \
  --bind /scinet/course/ss2019/3/8_bayesianmri/rstudio_auth.sh:/usr/lib/rstudio-server/bin/rstudio_auth.sh \
  --bind $SCRATCH/tmp:/tmp \
  --bind $SCRATCH:$SCRATCH \
  /scinet/course/ss2019/3/8_bayesianmri/RMINC-rserver-scinet-2019-06-26.img \
  rserver \
  --auth-none 0 \
  --auth-pam-helper-path rstudio_auth.sh \
  --www-port <your-port>
```

- Then navigate to localhost:8787/ in your browser, use your scinet username and `$RSTUDIO_PASSWORD$` 

---

# R Preamble

In this course we make heavy use of the `tidyverse` libraries and `purrr`
Some common functions we use:

```{r, eval = FALSE}
x %>% f # pipe, take `x` and apply `f` to it. `f(x)`

x %>% f(5, .) # pipe, take `x` and use it as the second argument to `f`.
              # `f(5, x)`

d %>% select(colA) # take the subset `d` thats includes just `colA`

d %>% rename(colB = colA) # rename `colA` to `colB` in a copy of d

d %>% mutate(colB = colA * 5) # make a new column `colB` that is
                              # 5 times the elements of `colA`

map(1:5, function(x) x * 2) # multiply each element of 1 through 5 by 2
```
---

# Classic Statistics

1. Decide on model
1. Apply model to every ROI/voxel separately
1. Compute p-values from per voxel hypothesis tests
1. Apply multiple comparisons corrections
1. Become famous

---

# Frequentist Null Hypothesis Testing

- To form conclusions in frequentism we typically lean on **null hypothesis testing**.
- Null hypotheses are parameter values for your model you'd like to disprove
- If your statistics (and more extreme statistics) would be very unlikely given your null model
  you reject the null hypothesis, and conclude that the null hypothesis is not correct.
- Choosing a threshold for this probability (e.g. 0.05) and rejecting when your p-value
  is below the threshold gives you a fixed probability of making a "Type I" error, 
  which conveniently is equal to your threshold.
- So if we reject all p-values when they are below 0.05 we have a 5% chance of
  rejecting when the null model is in fact true.
- If this is confusing, you're not alone, this is very hard to wrap your mind around.

---
class: center

# And now for Bayesianism!

---

# Why Bayesian Statistics?

Have you ever...

1. Been confused about what a p-value means?
1. Been frustrated that a difference in significance doesn't mean a significant difference?
1. Known some values for a parameter are impossible but been unable to use that to your advantage?
1. Wanted to ask more interesting questions than whether or not a parameter is or isn't zero?
1. Wanted to use information from the literature to improve your estimates?

---

# Why Bayesian Statistics?

Have you ever...

1. Been confused about what a p-value means?
1. Been frustrated that a difference in significance doesn't mean a significant difference?
1. Known some values for a parameter are impossible but been unable to use that to your advantage?
1. Wanted to ask more interesting questions than whether or not a parameter is or isn't zero?
1. Wanted to use information from the literature to improve your estimates?

Then Bayesian statistics might be right for you!

---

## How you ask?

1. **De-emphasize binary descisions.**
  Bayesians avoid null hypothesis tests, instead focusing on estimating their parameters of 
  interest, and reporting their uncertainty.
1. **Posterior Distributions**
  Bayesian analyses produce a distribution of possible parameter values (the posterior), that
  can be used to ask many interesting questions about values. E.g. what is the probability the
  effect in the hippocampus is larger than the effect in the anterior cingulate cortex.
1. **Prior Information**
  Bayesian analyses can use prior information. Bayesian analysis requires an *a priori* assessment
  of how likely certain parameters are. This can be vague (uninformative) or can precise (informative)
  and steer your analysis away from nonsensical results.

---

class: center
# Meet The Reverend

Reverend Thomas Bayes


![](Thomas_Bayes.gif)


---
class: middle

## Bayes' Theorem

- Bayes noticed this useful property for the probabilities for two events "A" and "B"

$$ \color{red}{P(A | B)} = \frac{{\color{blue}{P(B | A)}\color{orange}{P(A)}}}{\color{magenta}{P(B)}} $$
- $\color{red}{P(A|B)}$: The probability of A given that B happened
- $\color{blue}{P(B|A)}$: The probability of B given that A happened
- $\color{orange}{P(A)}$: The probability of A
- $\color{magenta}{P(B)}$: the probability of B

- Bayes did this in the context of the binomial distribution

---
class: middle
# But who's that behind him!

---
class: center

# It's Pierre-Simon Laplace

![](Pierre-Simon-Laplace.jpg)

---

## Bayesian Statistics

- Laplace generalized Bayes Theorem into it's modern form. While working on sex-ratios in French births.
- For light reading on the history of bayesianism consider reading [the theory that would not die](https://yalebooks.yale.edu/book/9780300188226/theory-would-not-die)

## Bayes in brief
- Start with some parameters $\theta$
- Collect some data $D$
- And deduce the probability of different values of $\theta$ given that you observed $D$
- Key difference between Bayesianism and Frequentism is that view that $\theta$ has an associated
  probability distribution. In frequentism $\theta$ is an unknown constant.

---

# Different Probabilities

- Frequentists believe that probabilities represent the long-run proportion of events
- Under this model $P(\theta)$ doesn't make much sense. 
- Ramsey and DeFinetti showed that probability can also represent degree of belief.
- Under this model $P(\theta)$ is an assesment of what you think the the parameter will be.
- For some of the philosophy underpinning bayesian reasoning consider reading 
  [Bayesian philosophy of science](http://www.laeuferpaar.de/Papers/BookFrame_v1.pdf)
  
---
class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} = \frac{{\color{blue}{P(D | \theta)}\color{orange}{P(\theta)}}}{\color{magenta}{\int P(D | \theta)P(\theta)d\theta}} $$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

**Normalizing Constant**: $\color{magenta}{\int P(D | \theta)P(\theta)d\theta}$

The probability of the data averaged over all possible parameter sets


---

class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} \propto \color{blue}{P(D | \theta)}\color{orange}{P(\theta)}$$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

---

class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} \propto \color{orange}{P(\theta)}\color{blue}{P(D | \theta)}$$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Pardon the re-ordering**

---
class: middle

# Posterior

$\color{red}{P(\theta|D)}$

- The goal of bayesian statistics
- The posterior is probability distribution over parameters.
- Depends on the data we observed.
- Can be used to answer interesting questions. For
example how likely is an effect between two biologically meaningful boundaries.

---
class: middle

# Prior

$\color{orange}{P(\theta)}$

- This is what we knew before the experiment. 
- The prior is also a probability distribution over parameters.
- Doesn't depend on the data we saw.
- Gives a probability for any value the parameters could take.

---
class: middle

# Likelihood

$\color{blue}{P(D | \theta)}$

- This is how probable our data is given a hypothetical parameter set
- The likelihood is a probability distribution over data (not parameters)
- Is still a function of parameters.

---

# In words

$$ \color{red}{P(\theta | D)} \propto \color{orange}{P(\theta)}\color{blue}{P(D | \theta)}$$

The <font color="red">probability of parameters given our data</font> is proportional to <font color="orange">how probable we
thought they were before</font> adjusted by <font color="blue">how well they agree with the data we saw</font>.

![](conjugate.png)

---

# A first example

- Let's revisit linear modelling from a frequentist and a bayesian stand-point.

$$ \textbf{y} = X\mathbf{\beta} + \mathbf{\epsilon} $$ 

We'll make our probabilistic views explicit

$$ \mathbf{\epsilon} \sim \mathcal{N}(0, \sigma) $$

$\epsilon$ is normally distributed with some unknown variance $\sigma$

---


# Frequentist interpretation

- In frequentism $\mathbf{\beta}$ is some fixed value.
- We can estimate standard errors for $\beta$ and get p-values
  (likelihoods) that each component of $\mathbf{\beta}$ is zero.
  
# Bayesian interpretation

- In bayesianism $\mathbf{\beta}$ is a random variable that we're trying
  to learn about.
- In order to do this we have specify our prior belief about $\mathbf{\beta}$
- If we say we know nothing about $\mathbf{\beta}$, we get identical estimates
  to frequentism
- For our model we'll say $\beta \sim \mathcal{N}(0,5)$

---

# Why now?

- Bayesian statistics has been around since Laplace. Why is it becoming popular now?

---

# Why now?

- Bayesian statistics has been around since Laplace. Why is it becoming popular now?
- **Probabilistic programming tools**
- Computers got a lot faster, and very smart people wrote very useful software
  for using bayesian stats.
- One important tool is Stan.

---

# Stan

- Stan is a modelling focused probabilistic programming language
- It makes it easy to express complicated models in code
- Has interfaces for R, python, and many more
- Backed by a team of very talented statisticians and programmers
- Uses automatic differentiation for very efficient sampling
- We're going to use Stan via a wrapper library called rstanarm

---

# Simulate some data

To see this software in action, we'll start with some simple simulated data.

```{r, fig.height = 4}
set.seed(20190627)
age <- rnorm(100)
brain_volume <- 3 * age + rnorm(100)
ex <- data.frame(brain_volume = brain_volume, age = age)

plot(age, brain_volume)
abline(0, 3)
```

---

# Analyze the frequentist way

```{r}
lmod <- lm(brain_volume ~ age, data = ex)
summary(lmod)
```

---

# The bayes way

- For this we'll use the package `rstanarm`

```{r}
suppressMessages(library(rstanarm))

bl <- stan_glm(brain_volume ~ age, data = ex,
                prior = normal(0, 5))
```

---

# rstanarm output

```{r}
bl
```

---

# Side-By-Side

We expect a slope of 3 and an intercept of 0

```{r}
coef(bl)
coef(lmod)
```

We can see they're giving almost the same answer, because the likelihood is much stronger than
the prior in the bayesian case.

---

# Let's look at the posterior

```{r, fig.height = 2.5}
suppressPackageStartupMessages(library(bayesplot))
mcmc_areas(as.matrix(bl), pars =  "age", prob = .95)
mcmc_areas(as.matrix(bl), pars = "(Intercept)", prob = .95)
```

---

# What's happening here?

```{r, eval = FALSE}
stan_glm(brain_volume ~ age, data = ex,
         prior = normal(0, 5))
```

- `rstanarm` is creating a posterior for us, but how?
- in most bayesian textbooks this is shown first analytically for simple models.
  **this is not what stan does**
- Stan *approximates* the posterior using samples
- Samples are generated with markov-chain monte carlo (MCMC)
- For more details on the technique see Michael Betancourt's [A conceptual introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)

---

# Did sampling work?

- MCMC techniques need to be checked to ensure they're generating good samples

```{r, fig.height = 3}
mcmc_trace(as.matrix(bl, pars = c("age", "(Intercept)")))
```

---

# Did sampling work?

```{r}
rhat(bl)
neff_ratio(bl) * 4000
```

- We had excellent sampling efficiency!
- We had no divergences (warning when we run the model)
- See [bayesplot visual diagnosis](https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html) for more.

---

# The posterior revisited

```{r}
bl_post <- as.matrix(bl)

str(bl_post[,"age"])
sum(bl_post[,"age"] > 3) / 4000
```

---

# Extra exploration

Maybe this will work, if not, try it at home!

```{r, eval = FALSE}
launch_shinystan(bl)
```

---

# Summary so far

- Bayes' Theorem lets us integrate new observations with prior knowledge/beliefs
- Bayesian statistics requires a prior distribution and a likelihood function and
  produces a posterior distribution
- Weak priors don't change results much.
- Posterior distributions are generally approximated with samples
- Samples allow easy conversion between properties and probabilities
- It's only a few commands to get started

---
class: center

# Now for some real data!


---
class: middle

# The dataset: ADNI

* baseline scans from ADNI2
* multi-site initiative to study Alzheimer's and Mild Cognitive Impairment

With much gratitude to Nikhil Bhagwat of the CoBrA Lab (PI: Chakravarty) for providing us with processed data.

---

# MAGeT - the algorithm

![MAGeT figure](MAGeT-fig.png)

Chakravarty MM, Steadman P, van Eede MC, Calcott RD, Gu V, Shaw P, Raznahan A, Collins DL, Lerch JP. Performing label-fusion-based segmentation using multiple automatically generated templates. Hum Brain Mapp. 2013 Oct;34(10):2635–54.

---

# MAGeT - the atlas


```{r, echo=F, out.height="400px"}
knitr::include_graphics("hippoatlas.png")
```

.small[
Pipitone J, Park MTM, Winterburn J, Lett TA, Lerch JP, Pruessner JC, Lepage M, Voineskos AN, Chakravarty MM, Alzheimer’s Disease Neuroimaging Initiative. Multi-atlas segmentation of the whole hippocampus and subfields using multiple automatically generated templates. Neuroimage. 2014 Nov 1;101:494–512.

Winterburn JL, Pruessner JC, Chavez S, Schira MM, Lobaugh NJ, Voineskos AN, Chakravarty MM. A novel in vivo atlas of human hippocampal subfields using high-resolution 3 T magnetic resonance imaging. Neuroimage. 2013 Jul 1;74:254–65.
]
---

# Load the data

Read in the processed files

```{r}
suppressMessages(library(tidyverse))
adni <- read.csv("ADNI2_BL_MAGeT_Hippocampus_subfields.csv")
names(adni)
```

---

# Rename columns and fix the diagnosis label

```{r}
library(forcats)
adni <-
  adni %>%
  rename(age = AGE,
         id = PTID,
         dx = DX_bl,
         sex = PTGENDER
         ) %>%
  mutate(dx=fct_relevel(
           dx, "CN", "SMC", "EMCI", "LMCI", "AD"
         ))
```

---

# Data organization

Make the dataframe long rather than wide

```{r}
adniLong <- adni %>% 
  select(-X) %>% 
    gather(structure, volume, L_CA1:R_Mam)
adniLong %>% 
  select(id, dx, structure, volume) %>% sample_n(5)
```


---

# A quick look at the data

```{r, fig.height=5.5}
library(ggplot2)
ggplot(adniLong) + aes(x=dx, y=age) + geom_boxplot()
```

---

# A quick look at hippocampal volume

```{r, tidy=FALSE, fig.height=5}
adniLong %>% group_by(id) %>% 
  summarize(dx=dx[1], sex=sex[1], HPC=sum(volume)) %>%
  ggplot() + aes(dx, HPC, colour=sex) + geom_boxplot()
```

---

# Zoom in on a single structure

```{r}
adniLong %>% filter(structure == "L_CA1") %>%
  lm(volume ~ age+sex+dx, .) %>% summary
```

---

# Interpreting these results

- After fitting our models we're left with:

1. coefficient estimates
1. t-statistics
1. p-values

- We know if our model assumptions are satisfied our t-statistics
  have a known distribution. 
- From this distribution we can figure out the probability of t-statistics
  as large or larger than the one we observed (p-value)

---

# Back to Bayes

- Let's look at the effect of diagnosis on CA1 volume

```{r}
scale_vec <- function(x) (x - mean(x)) / sd(x)
ca1_ex <-
  adni %>%
  mutate(volume = scale_vec(L_CA1 + R_CA1))
```

- Scaling here helps improve model fitting, an allows you to interpret parameters in terms of whole sample standard deviations.
- For details on why this is a good idea consider checking out Gelman and Hill's [Data Analysis Using Regression and Multilevel/Hierarchical Models](http://www.stat.columbia.edu/~gelman/arm/)

---

# Model

$$\text{volume} = \beta_{D}D + \epsilon$$
- We're not going to estimate an intercept for this model, fitting instead the mean volume for each diagnosis
- in a R formula it will look like

`volume ~ -1 + dx`

- The -1 removes the intercept

---

# Fit

```{r}
invisible(
  capture.output({
    ca1_mod <- stan_glm(volume ~ -1 + dx, data = ca1_ex)
  }))

ca1_mod
```

---

So we see that the effect of each diagnosis has on CA1 volume. Let's visualize the posterior for these effects

```{r, fig.height = 5}
mcmc_areas(as.matrix(ca1_mod), regex_pars = "dx.*")
```

---

```{r}
table(ca1_ex$dx)
```

So we can see some signs that the posterior width is driven in part by sample size, nice.

---

# Sample magic

First let's extract the posterior

```{r}
ca1_post <- as.matrix(ca1_mod)
```

What do we think the probability is that AD patients have smaller CA1s than controls? Easy to answer with the posterior!

```{r}
mean(ca1_post[,"dxCN"] > ca1_post[,"dxAD"])
```

So we're pretty sure about this. 

---

# But how big is the difference?

```{r, fig.height = 4}
mcmc_areas(ca1_post[,"dxCN", drop = FALSE] - ca1_post[,"dxAD"])
```

---

# A massively univariate analysis

Here we'll fit a simple linear model (age + sex + diagnosis) at each structure.

```{r}
library(broom)
models <-
  adniLong %>%
  split(.$structure) %>%
  map(~lm(volume ~ age+sex+dx, .)) %>%
  map_dfr(tidy, .id='roi')

head(models, 14)
```
---

# better overview

```{r}
rTable <-
  models %>%
  filter(startsWith(term, "dx")) %>%
  select(roi, term, statistic) %>%
  spread(term, statistic) 
```

---
# better overview
```{r}
rTable
```

---

# better overview

```{r}
DT::datatable(rTable %>% remove_rownames() %>% 
  column_to_rownames("roi") %>% round(2), options=list(pageLength=7))
```

---

# Dealing with Many Tests

- If you're testing a lot of hypotheses, a 5% chance of making a mistake adds up
- After 14 tests you have a better than a 50/50 chance of having made at least one mistake
- How do we control for this?
- Two main approaches Family-Wise Error Rate (FWER) control and False-Discovery Rate (FDR) control.

---

# FWER

- In family-wise error rate control, we try to limit the chance we will make at least one
  type I error.
- Classic example, Bonferroni
- Quite conservative, so in neuroimaging we tend to use False Discovery Rate control.

---

# FDR

- Instead of trying to control our chances of making at least one mistake, let's try to control the
  fraction of mistakes we make.
- To do this we employ the Benjamini-Hochberg procedure.
- The Benjamini-Hochberg procedure turns our p-values in q-values. Rejecting all q-values below some
  threshold controls the expected number of mistakes.
- For example if we reject all hypotheses with q < 0.05, we expect about 5% of our results to be
  false discoveries (type I errors). 
- If we have 100's or more tests we can accept a few mistakes in the interest of finding the
  important results.

---

# multiple comparisons - omnibus FDR

```{r}
pTable <- models %>% 
  filter(startsWith(term, "dx")) %>%
  select(roi, term, p.value) %>%
  spread(term, p.value) %>%
  remove_rownames() %>%
  column_to_rownames("roi") %>%
  as.matrix()
qTable <- pTable
qTable[,] <- p.adjust(pTable, 'fdr')
```

---

# multiple comparisons - omnibus FDR

```{r}
qTable
```

---

# Summary so far

- On real data we typically do massively univariate analysis
- Massively univariate analyses treat each structure as independent
- Multiple comparison corrections are needed when you have many independent
  hypothesis tests
- Converting to bayesian analyses for single structures is about as simple
  as changing `lm` to `stan_glm`

---

# Are we missing something?

- Before we go and switch to bayesian massively univariate analysis let's
  think for a moment
- In the massively univariate approach we ignore that structures are related!
- We should be able to use information about the other structures to inform
  our analysis of each structure.
- We can do this by adding another level of model, complete with more priors
- This is called multilevel or hierarchical modelling
- The brain can be viewed as hierarchically organized
- So we should capitalize on all this extra information!

---

# Hippocampal anatomical hierarchy

For the rest of the course we're going to focus on the following hierarchy

- Hippocampal Formation
    - Grey Matter
        - CA1
        - CA2/CA3
        - CA4/DG
        - subiculum
        - stratum
        - Mammilary bodies
    - White Matter
        - Alveus
        - Fimbria
        - Fornix

---

# Hippocampal anatomical hierarchy

Some code to generate a data representation of this tree:

```{r}
library(data.tree)
hpc <- Node$new("HPC")
gm <- hpc$AddChild("GM")
ca1 <- gm$AddChild("CA1")
lca1 <- ca1$AddChild("left CA1")
lca1$volumes <- adni$L_CA1
rca1 <- ca1$AddChild("right CA1")
rca1$volumes <- adni$R_CA1
ca23 <- gm$AddChild("CA2CA3")
lca23 <- ca23$AddChild("left CA2CA3")
lca23$volumes <- adni$L_CA2CA3
rca23 <- ca23$AddChild("right CA2CA3")
rca23$volumes <- adni$R_CA2CA3
ca4 <- gm$AddChild("CA4DG")
lca4 <- ca4$AddChild("left CA4DG")
lca4$volumes <- adni$L_CA4DG
rca4 <- ca4$AddChild("right CA4DG")
rca4$volumes <- adni$R_CA4DG
subiculum <- gm$AddChild("subiculum")
lsubiculum <- subiculum$AddChild("left subiculum")
lsubiculum$volumes <- adni$L_subiculum
rsubiculum <- subiculum$AddChild("right subiculum")
rsubiculum$volumes <- adni$R_subiculum
stratum <- gm$AddChild("stratum")
lstratum <- stratum$AddChild("left stratum")
lstratum$volumes <- adni$L_stratum
rstratum <- stratum$AddChild("right stratum")
rstratum$volumes <- adni$R_stratum
mam <- gm$AddChild("Mammillary bodies")
lmam <- mam$AddChild("left Mammillary bodies")
lmam$volumes <- adni$L_Mam
rmam <- mam$AddChild("right Mammillary bodies")
rmam$volumes <- adni$R_Mam
wm <- hpc$AddChild("WM")
alveus <- wm$AddChild("Alveus")
lalveus <- alveus$AddChild("left Alveus")
lalveus$volumes <- adni$L_Alv
ralveus <- alveus$AddChild("right Alveus")
ralveus$volumes <- adni$R_Alv
fimbria <- wm$AddChild("Fimbria")
lfimbria <- fimbria$AddChild("left Fimbria")
lfimbria$volumes <- adni$L_Fimb
rfimbria <- fimbria$AddChild("right Fimbria")
rfimbria$volumes <- adni$R_Fimb
fornix <- wm$AddChild("Fornix")
lfornix <- fornix$AddChild("left Fornix")
lfornix$volumes <- adni$L_Fornix
rfornix <- fornix$AddChild("right Fornix")
rfornix$volumes <- adni$R_Fornix
```

---

# Hippocampal anatomical hierarchy

```{r}
hpc
```

---
# Aggregate up the tree

We have all of the structure volumes, but we need to add them up to
get the volume of the interior nodes.

```{r}
hpc$Do(function(x){
  x$volumes <- Aggregate(x, "volumes", rowSums)
}, traversal="post-order", filterFun=isNotLeaf)

hpc$Do(function(x) {
  x$meanVolume <- mean(x$volumes)
})
```

---
# What does it look like?

```{r, echo = FALSE, eval = FALSE}
library(htmlwidgets)
library(networkD3)
nd3 <- igraph_to_networkD3(data.tree::as.igraph.Node(hpc))
nd3$links$thickness <- hpc$Get("meanVolume")[nd3$links$target + 1]

# I screenshot this manually because networkD3 interacts strangely with xaringan (remark.js)
onRender(sankeyNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', Target = 'target', 
              NodeID = 'name', Value = 'thickness', height = 600
            , width = 900, fontSize = 18, margin = 0)
         , 'function(el, d, js){ el.onmouseenter = function(e){ e.target.firstChild.setAttribute("viewBox", ""); }; }')
```

![](sankey.png)

---

# Tree graph

```{r, fig.height=4}
library(treemap)
ToDataFrameTable(hpc, "pathString", "meanVolume", "name") %>% 
  mutate(path=strsplit(pathString, "/"), 
         struct=map_chr(path, ~ .x[3]), 
         tissue=map_chr(path, ~ .x[2])) %>% 
  select(struct, tissue, name, meanVolume) %>% 
  treemap(index=c("tissue", "struct"), vSize="meanVolume")
```

---

# What scale of analysis do I choose?

- What scale you choose to analyze can have a large impact
  on your results. 
- Should I use all leaf nodes? What if some are small/noisy?
- Should I merge left and right hemispheres?
- Do I care about a whole brain effect?
- One way to approach this is to analyze at all scales.

---
# Statistics on the tree

Here's how to fit linear models at each scale

```{r}
hpc$Do(function(x){
  adni$volumes <- x$volumes
  x$stats <- 
    lm(volumes ~ age+sex+dx, adni) %>%
    tidy() %>%
    filter(startsWith(term, "dx")) %>%
    select(term, estimate, statistic, p.value)
})
```
---

# Statistics on the tree

```{r}
print(hpc, AD=function(x) 
  x$stats %>% filter(term=="dxAD") %>% select(statistic) )
```

---

# Statistics on the tree

```{r}
print(hpc, AD=function(x) 
  x$stats %>% filter(term=="dxAD") %>% 
    select(statistic),
  filterFun=isNotLeaf )
```

---

# Statistics on the tree

```{r, fig.height=3}
hpc$Do(function(x){
  x$ad <- x$stats %>% filter(term=="dxAD") %>% select(statistic)
    #-log10(x$stats %>% filter(term=="dxAD") %>% 
    #               select(p.value))
})

ToDataFrameTable(hpc, "pathString", "meanVolume", "name", "ad") %>% 
  mutate(path=strsplit(pathString, "/"), 
         struct=map_chr(path, ~ .x[3]), 
         tissue=map_chr(path, ~ .x[2])) %>% 
  select(struct, tissue, name, meanVolume, ad) %>% 
  treemap(index=c("tissue", "struct"), vSize="meanVolume", 
    vColor="ad", type="value")
```
---

# Looking at many structures 

---
# Data preparation

```{r}
hpc$Do(function(x){
  x$normVolumes <- scale(x$volumes)
})

hpcSym <- Clone(hpc)
Prune(hpcSym, isNotLeaf)
data <- hpcSym$Get("normVolumes", filterFun=isLeaf)
colnames(data) <- hpcSym$Get("name", filterFun = isLeaf)
adniLongNorm <- adni %>% 
  mutate(age=age/10) %>% 
  select(id:dx) %>% 
  cbind(data) %>% 
  # sample_n(100) %>% # to make it not last an ice-age
  gather(structure, volume, CA1:Fornix) 
```

---
# The model

```{r, eval=FALSE}
flathierarchy <- stan_lmer(
  volume ~ age+sex+dx + (1|id) + 
           (age+sex+dx|structure), 
  adniLongNorm , chains=3, cores=3, adapt_delta = 0.99)
```

The model takes a while to run, unless you specified sample_n to a relatively small number. So just load the output from an earlier, saved run instead.

```{r}
flathierarchy <- readRDS("flathierarchy.Rds")
```

---
# The model
```{r, eval=FALSE}
flathierarchy <- stan_lmer(
  volume ~ age+sex+dx + (1|id) +
           (age+sex+dx|structure), 
  adniLongNorm , chains=3, cores=3, adapt_delta = 0.99)
```

What is this model doing?

* `volume ~ age + sex + dx`: the same model we saw before
* `(1|id)`: allow for variation across individuals; i.e. those with a larger CA1 are also likely to have a larger DG
* `(age+sex+dx|structure)`: repeat of the model, but now grouped by structure.

--

Huh?

--

* the main effects (first part) are across all structures
* for each structure, the deviation from the main effect is estimated

---

# The magic of pooling

How do we work with multiple brain structures at the same time? By pooling. Here's what that could mean:

--

**Complete pooling:** we ignore that we have multiple structures, and sum them. I.e. a single model across overall hippocampal volume.

--

**No pooling:** we compute a separate model for each structure. What happens in CA1 has no bearing on what happens in CA2/CA3. This is what we do with our massively univariate models. You need to control for multiple comparisons by widening your confidence intervals.

--

**Partial pooling:** what happens in CA1 will _somewhat_ influence what happens in CA2/CA3, in that the model will share information across structures. These are our hierarchical Bayesian models.

--

Partial pooling, once more: the prior for each structure is the pooled estimated of all structures. The stronger the data for each structure (i.e. the likelihood), the less the estimate will shrink towards the pooled estimates. And the pooled estimates are themselves determined from the data.

--

And since all structures share information, you need not control for multiple comparisons. Shrinkage replaces widening of confidence intervals.

---

# Wait, I've seen this before

`volume ~ age+sex+dx + (1|id) + (age+sex+dx|structure)`

- If you're thinking this looks familiar, you're probably right.
- This *is* the same syntax as mixed-effects models fit with lme4
- Conveniently, this *is* a linear mixed-effects model
- The difference between this and the model you would fit with lme4
  is the priors, and the fact you get posterior distributions


---
# Covariance priors

So if the prior for each structure is estimated from the data, we don't need a prior? Not quite - we have to decide how pooling happens.

--

This is our model from before:

$y = \alpha + X\beta + Zb + \epsilon$

* where $X$ is the matrix of predictors for the main effect, age+sex+dx in our model.
* and where $Z$ is the matrix that encodes deviation in the predictors across groups, or structures in our case, which we specified as (age+sex+dx|structure) and (1|id).

--

The likelihood is thus

$y \sim \mathcal{N} (\alpha + X\beta + Zb, \sigma^2 I)$

* intercept and coefficients common across structures
* deviations in intercepts and coefficients that vary across structures

---
# Covariance priors

Back to the likelihood:

$y \sim \mathcal{N} (\alpha + X\beta + Zb, \sigma^2 I)$

* intercept and coefficients common across structures
* deviations in intercepts and coefficients that vary across structures

--

We now need priors on parameters. We maintain the simple assumption that $b \sim \mathcal{N} (0,\Sigma)$ - i.e. the deviation for each structure is centred at zero with some variance.

--

But now we need to state our prior beliefs about $\Sigma$ in addition to $\alpha$, $\beta$, and $\sigma$. 

--

Our prior for varying slopes and intercepts is a "zero-mean random vector following a multivariate Gaussian distribution with an unknown covariance matrix to be estimated"

--

More detail here: [http://mc-stan.org/rstanarm/articles/glmer.html]

---
# Taking a peek

```{r}
summary(flathierarchy, digits=3)
```

---
# Quick diagnostics: Rhat

```{r, fig.height=4}
mcmc_rhat_hist(rhat(flathierarchy))
```

---
# More diagnostics: trace plots

We'll need data in matrix form for this and many future ops
```{r}
flathierarchym <- as.matrix(flathierarchy)
```

```{r, fig.height=5}
mcmc_trace(flathierarchym[,1:7])
```

---

# Summary so far

- We can incorporate information that structures are related by using a 
  multi-level (mixed-effects like) model.
- The model is the same in structure to frequentist models, differing only
  in the priors we use.
- We can fit these using `stan_lmer` anywhere we would use `lmer`
- Diagnostics are looking good

---

# Now playing with the posterior distributions

---

# The main effects

```{r, fig.height=6, fig.width=10}
mcmc_intervals(flathierarchym[,1:7])
```

---
# Effect across structures
```{r, fig.height=6, fig.width=10}
mcmc_intervals(flathierarchym, 
  pars="dxAD", regex_pars = 'dxAD structure')
```

---
# Effect across structures
```{r, fig.height=6, fig.width=10}
mcmc_areas_ridges(flathierarchym, 
  pars="dxAD", regex_pars = 'dxAD structure')
```

---
# Summed effects

```{r, fig.height=5.5, fig.width=10}
vars <- colnames(flathierarchym)
ADsamples <- flathierarchym[,grep('dxAD structure', vars)]
mcmc_areas_ridges(ADsamples + flathierarchym[,"dxAD"])
```

---
# Probability calculations

What is the probability that CA1 was more affected than CA2/CA3 in AD?

```{r}
mean(flathierarchym[,'b[dxAD structure:CA1]'] < 
       flathierarchym[,'b[dxAD structure:CA2CA3]'])
```

Not very likely

```{r}
quantile(flathierarchym[,'b[dxAD structure:CA1]'] - 
       flathierarchym[,'b[dxAD structure:CA2CA3]'])
```

---
# Probability calculations
What is the probability that the subiculum was more affected than CA1 in AD?

```{r}
mean(flathierarchym[,'b[dxAD structure:subiculum]'] < 
       flathierarchym[,'b[dxAD structure:CA1]'])
```

Almost certainly

```{r}
quantile(flathierarchym[,'b[dxAD structure:subiculum]'] - 
       flathierarchym[,'b[dxAD structure:CA1]'])
```

---
# All pairs of probabilities

```{r}
ADvars <- grep("dxAD structure", vars)
probcomps <- matrix(nrow=length(ADvars), ncol=length(ADvars))
for (i in 1:length(ADvars)) {
  for (j in 1:length(ADvars)) {
    probcomps[i,j] <- mean(flathierarchym[,ADvars[i]] < 
                             flathierarchym[,ADvars[j]])
  }
}
```

```{r}
pnames <- sub('.+structure:(.+)\\]', '\\1', 
              colnames(flathierarchym[,ADvars]))
rownames(probcomps) <- pnames
colnames(probcomps) <- pnames
```

---
# All pairs of probabilities

```{r, fig.height=5.5}
library(pheatmap)
pheatmap(probcomps, display_numbers=T)
```


---

# Estimating shrinkage

First, compute the least squares estimates
```{r}
lmEstimates <- adniLongNorm %>%
  split(.$structure) %>%
  map(~lm(volume ~ age+sex+dx, .)) %>%
  map_dfr(tidy, .id='roi') %>%
  filter(term == "dxAD") %>%
  select(roi, estimate)
```

---

# Estimating shrinkage

Next, compute the medians from the Hierarchical Bayesian model

```{r}
sweptADsamples <- flathierarchym[,ADvars] + flathierarchym[,"dxAD"]
lmAndBayes <- sweptADsamples %>% 
  as_data_frame %>% 
  gather(term, value) %>% 
  group_by(term) %>% 
  summarize(median=median(value)) %>% 
  mutate(term=sub('.+structure:(.+)\\]', '\\1', term)) %>% 
  mutate(term=sub('_', ' ', term)) %>%
  inner_join(lmEstimates, by=c("term" = "roi")) 
```


---

# Estimating shrinkage

```{r}
lmAndBayes %>% mutate(diff=abs(median - estimate))
```

---

# Estimating shrinkage

```{r, fig.height=4}
library(ggrepel)
ggplot(lmAndBayes) + 
  aes(median, estimate) + 
  geom_point() + 
  geom_abline(aes(intercept=0, slope=1)) + 
  geom_text_repel(aes(label=term)) + 
  xlab("Hierarchical Bayesian") + 
  ylab("Least squares")
```

---
# Estimating shrinkage across all terms

```{r}
structvars <- grep('b.+structure', vars, value = T)
mainterms <- sub('b\\[(.+) str.+', '\\1', structvars)

structswmain <- flathierarchym[,structvars] + flathierarchym[,mainterms]

bOut <- structswmain %>% 
  as.data.frame %>% 
  gather(mterm, value) %>%
  group_by(mterm) %>% 
  summarize(median=median(value)) %>%
  mutate(roi = sub('.+structure:(.+)\\]', '\\1', mterm),
         roi = sub('_', ' ', roi),
         term = sub('b\\[(.+) str.+', '\\1', mterm)) %>%
  select(median, roi, term)


lOut <- adniLongNorm %>%
  split(.$structure) %>%
  map(~lm(volume ~ age+sex+dx, .)) %>%
  map_dfr(tidy, .id='roi') %>%
  select(estimate, roi, term)

blOut <- inner_join(bOut, lOut) 
```

---

# Estimating shrinkage across all terms

```{r}
ggplot(blOut) + 
  aes(median, estimate, colour=term) + 
  geom_point() +
  geom_abline(aes(intercept=0, slope=1))
```

---

# Estimating shrinkage across all terms

```{r, fig.height=5}
ggplot(blOut) + 
  aes(median, estimate, colour=roi) + 
  geom_point() +
  geom_abline(aes(intercept=0, slope=1)) +
  xlab("Hierarchical bayes") +
  ylab("Least squares") +
  facet_wrap(~term, scales="free")
```

---

# What about the MCI groups?

Remember, nothing survived FDR comparisons ...

```{r}
mcmc_intervals(structswmain, regex_pars="EMCI")
```

---

# And LMCI

```{r}
mcmc_intervals(structswmain, regex_pars="LMCI")
```

---
# A reminder about LMCI FDR

```{r}
hpcSym$Do(function(x){
  adni$volume <- x$normVolumes
  x$stats <- tidy(lm(volume ~ age+sex+dx, adni))
  x$LMCI <- x$stats %>% filter(term=="dxLMCI") %>% select(p.value)
})
round(p.adjust(
  hpcSym$Get("LMCI", filterFun = isLeaf), 'fdr'), 
  3)
```

---

# Comparing probabilities

```{r}
LMCIprobs <- structswmain %>% 
  as.data.frame %>% 
  gather(mterm, value) %>%
  group_by(mterm) %>% 
  summarize(median=median(value),
            prob=case_when(
              median > 0 ~ 1 - mean(value>0),
              median < 0 ~ 1 - mean(value<0)
            )) %>%
  mutate(roi = sub('.+structure:(.+)\\]', '\\1', mterm),
         roi = sub('_', ' ', roi),
         term = sub('b\\[(.+) str.+', '\\1', mterm)) %>%
  filter(term=="dxLMCI") %>%
  select(roi, median, prob)

LMCIq <- hpcSym$Get("LMCI", filterFun = isLeaf) %>% 
  as.data.frame() %>% 
  gather(roi, p) %>%
  mutate(q=p.adjust(p, 'fdr'),
         roi=sub('\\.', ' ', roi))
```

---

# Comparing probabilities

```{r}
inner_join(LMCIprobs, LMCIq) %>% mutate_at(vars(median:q), round, 3)
```

---

# A more complex hierarchy

Create an ancestor variable from the tree, in this case corresponding to WM or GM

```{r}
hpc$Do(function(x){
  x$normVolumes <- scale(x$volumes)
})
hpc$Do(function(x){
  x$tissue <- x$parent$name
}, filterFun = isLeaf)

hpcSym <- Clone(hpc)
Prune(hpcSym, isNotLeaf)

hpcSym$Do(function(x){
  x$tissue <- x$parent$name
}, filterFun = isLeaf)

data <- hpcSym$Get("normVolumes", filterFun=isLeaf)
colnames(data) <- hpcSym$Get("name", filterFun = isLeaf)

tissueTable <- ToDataFrameTable(hpcSym, "name", "tissue")
```

---
# A more complex hierarchy

```{r}
adniLongNorm <- adni %>% mutate(age=age/10) %>% 
  select(id:dx) %>% 
  cbind(data) %>% 
  #sample_n(50) %>%
  gather(structure, volume, CA1:Fornix) %>%
  left_join(tissueTable, by=c("structure" = "name"))
  
adniLongNorm %>% sample_n(5)
```

---
# A more complex hierarchy

Allow for structure within tissue

```{r, eval=F}
# this takes a long time unless a small subset of the data is used
twolevelhierarchy <- 
  stan_lmer(volume ~ age+sex+dx + (1|id) + 
              (age+sex+dx|tissue/structure), 
            adniLongNorm , chains=3, cores=3, adapt_delta = 0.99)
```

```{r}
# loads the precomputed output from the above command
twolevelhierarchy <- readRDS("twolevelhierarchy.Rds")
twolevelhierarchym <- as.matrix(twolevelhierarchy)
```

---
# A look at the new encoding

Remember that you are encoding the difference from the level one up

```{r, fig.height=4}
mcmc_intervals(twolevelhierarchym, regex_pars="b\\[dxAD", pars="dxAD")
```

--

In this case, the extra hierarchy level has little effect

---
# A look at plots - LMCI as before
.small[
```{r, fig.height=2.5}
suppressMessages(library(gridExtra))
vars <- colnames(twolevelhierarchym)
structvars <- grep('b.+structure', vars, value = T)

correspondingtissue <- sub('b\\[(.+) structure:tissue:.+:(.+)\\]', 'b[\\1 tissue:\\2]', structvars)
correspondingmain <- sub('b\\[(.+) structure:tissue:.+:(.+)\\]', '\\1', structvars)

twolevelstructs <- twolevelhierarchym[,structvars] + 
  twolevelhierarchym[,correspondingtissue] +
  twolevelhierarchym[,correspondingmain]

ylabels <- tissueTable %>% arrange(name) %>% select(name)

grid.arrange(
  mcmc_intervals(twolevelstructs, regex_pars = "dxLMCI") + 
    ggtitle("Two level") + scale_y_discrete(labels=ylabels$name),
  mcmc_intervals(structswmain, regex_pars = "dxLMCI") +
    ggtitle("Flat") + scale_y_discrete(labels=ylabels$name),
  ncol=2)
```
]

---

# Conclusion

- I hope you learned a little bit about how to analyze neuroimaging data with R
- I hope you appreciate that more complicated models with the right priors help
  us capitalize on rich hierarchical brain data
- I hope you learned some useful R tricks along the way

---

# Installing RMINC

- RMINC provides many convenience functions for working with hierarchically structured volume data. 
- The lead developers are responsive on github and willing to offer help if needed.
- Full install guide can be found at https://github.com/Mouse-Imaging-Centre/RMINC/blob/master/INSTALL

---

# Linux install:

1. Get the minc-toolkit-v2 (https://bic-mni.github.io/)
1. Source the configure script `source /opt/minc/1.9.16/minc-toolkit-config.sh` or where-ever your minc-toolkit version was installed
1. Set your `MINC_PATH` environment variable `export MINC_PATH=/opt/minc/1.9.16/`
1. Open an R session
1. Install `devtools` if you have not already `install.packages("devtools")`
1. Install `RMINC` with 
```{r, eval = FALSE}
devtools::install_github("Mouse-Imaging-Centre/RMINC", 
  dependencies = c("Depends", "Imports", "LinkingTo",
                   "Suggests","Enhances"))
```

---

# Mac Install

Mac install is very similar, however fortran libraries are necessary, and so you will likely need the
package manager [brew](https://brew.sh/). From there you will need to run `brew install gcc`

Then find a directory that looks like:

```{r, engine = "bash", eval = FALSE}
/usr/local/Cellar/gcc/7.*.*/lib/gcc/7
```

this needs to be added to your R build variables (makevars)

```{r, eval = FALSE, engine = "bash"}
mkdir $HOME/.R/ ## Ignore error here
touch $HOME/.R/Makevars
echo 'FLIBS=-L/usr/local/Cellar/gcc/7.1.0/lib/gcc/7' >> \
  $HOME/.R/Makevars
```

If this succeeds, go ahead and follow the linux instructions.


